{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An example of link prediction using negative and positive samples.\n",
    "# Ported from https://docs.dgl.ai/tutorials/blitz/4_link_predict.html#sphx-glr-tutorials-blitz-4-link-predict-py\n",
    "# See the comparison paper https://arxiv.org/pdf/2102.12557.pdf for more details\n",
    "\n",
    "using Flux\n",
    "using Flux: onecold, onehotbatch\n",
    "using Flux.Losses: logitbinarycrossentropy\n",
    "using GraphNeuralNetworks\n",
    "using MLDatasets: PubMed\n",
    "using Statistics, Random, LinearAlgebra\n",
    "using CUDA\n",
    "CUDA.allowscalar(false)\n",
    "\n",
    "# arguments for the `train` function \n",
    "Base.@kwdef mutable struct Args\n",
    "    η = 1.0f-3             # learning rate\n",
    "    epochs = 200          # number of epochs\n",
    "    seed = 17             # set seed > 0 for reproducibility\n",
    "    usecuda = true      # if true use cuda (if available)\n",
    "    nhidden = 64        # dimension of hidden features\n",
    "    infotime = 10      # report every `infotime` epochs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "  num_nodes: 19717\n",
       "  num_edges: 88648\n",
       "  ndata:\n",
       "\tval_mask = 19717-element BitVector\n",
       "\ttargets = 19717-element Vector{Int64}\n",
       "\ttest_mask = 19717-element BitVector\n",
       "\tfeatures = 500×19717 Matrix{Float32}\n",
       "\ttrain_mask = 19717-element BitVector"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_bidirected(g) = true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training on CPU\n",
      "└ @ Main c:\\Users\\HP\\Desktop\\VicWorks03082024\\eSGCe_PubMD.ipynb:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_self_loops(g) = false\n",
      "has_multi_edges(g) = false\n",
      "mean(degree(g)) = 4.496018664096972\n",
      "Epoch: 0  (train_loss = 0.6920696f0, train_acc = 0.500463752130753)  (test_loss = 0.69208866f0, test_acc = 0.5003384476534296)\n",
      "Epoch: 10  (train_loss = 0.6666491f0, train_acc = 0.5)  (test_loss = 0.66596544f0, test_acc = 0.5)\n",
      "Epoch: 20  (train_loss = 0.6477221f0, train_acc = 0.5)  (test_loss = 0.646282f0, test_acc = 0.5)\n",
      "Epoch: 30  (train_loss = 0.63259023f0, train_acc = 0.5)  (test_loss = 0.6312668f0, test_acc = 0.5)\n",
      "Epoch: 40  (train_loss = 0.6168112f0, train_acc = 0.5)  (test_loss = 0.6163021f0, test_acc = 0.5)\n",
      "Epoch: 50  (train_loss = 0.5971608f0, train_acc = 0.5122330291787827)  (test_loss = 0.5957484f0, test_acc = 0.5119584837545126)\n",
      "Epoch: 60  (train_loss = 0.56372976f0, train_acc = 0.6297127243557605)  (test_loss = 0.5630997f0, test_acc = 0.6262409747292419)\n",
      "Epoch: 70  (train_loss = 0.534999f0, train_acc = 0.700015040609646)  (test_loss = 0.53804153f0, test_acc = 0.6896435018050542)\n",
      "Epoch: 80  (train_loss = 0.5297806f0, train_acc = 0.7106688057755941)  (test_loss = 0.53237814f0, test_acc = 0.699345667870036)\n",
      "Epoch: 90  (train_loss = 0.5150366f0, train_acc = 0.7141908152010428)  (test_loss = 0.52024734f0, test_acc = 0.7028429602888087)\n",
      "Epoch: 100  (train_loss = 0.5037504f0, train_acc = 0.7211846986864534)  (test_loss = 0.5063391f0, test_acc = 0.7149142599277978)\n",
      "Epoch: 110  (train_loss = 0.4875753f0, train_acc = 0.742554898225208)  (test_loss = 0.4907459f0, test_acc = 0.734995487364621)\n",
      "Epoch: 120  (train_loss = 0.47637066f0, train_acc = 0.7559159731274441)  (test_loss = 0.48200834f0, test_acc = 0.7497743682310469)\n",
      "Epoch: 130  (train_loss = 0.473413f0, train_acc = 0.7535721447909356)  (test_loss = 0.4790328f0, test_acc = 0.7506768953068592)\n",
      "Epoch: 140  (train_loss = 0.46881056f0, train_acc = 0.7542239045422641)  (test_loss = 0.4738213f0, test_acc = 0.7524819494584838)\n",
      "Epoch: 150  (train_loss = 0.46542546f0, train_acc = 0.7527574451017748)  (test_loss = 0.4687707f0, test_acc = 0.7513537906137184)\n",
      "Epoch: 160  (train_loss = 0.46131086f0, train_acc = 0.7549634011831946)  (test_loss = 0.46364135f0, test_acc = 0.7553023465703971)\n",
      "Epoch: 170  (train_loss = 0.45417482f0, train_acc = 0.758623282863732)  (test_loss = 0.45964107f0, test_acc = 0.7555279783393501)\n",
      "Epoch: 180  (train_loss = 0.45375228f0, train_acc = 0.7613556602827635)  (test_loss = 0.45746976f0, test_acc = 0.7559792418772564)\n",
      "Epoch: 190  (train_loss = 0.45228636f0, train_acc = 0.7572696279955881)  (test_loss = 0.45660546f0, test_acc = 0.7560920577617329)\n",
      "Epoch: 200  (train_loss = 0.4526972f0, train_acc = 0.7590243657876266)  (test_loss = 0.45626682f0, test_acc = 0.756317689530686)\n"
     ]
    }
   ],
   "source": [
    "# We define our own edge prediction layer but could also \n",
    "# use GraphNeuralNetworks.DotDecoder instead.\n",
    "struct DotPredictor end\n",
    "\n",
    "function (::DotPredictor)(g, x)\n",
    "    z = apply_edges((xi, xj, e) -> sum(xi .* xj, dims = 1), g, xi = x, xj = x)\n",
    "    # z = apply_edges(xi_dot_xj, g, xi=x, xj=x) # Same with built-in method\n",
    "    return vec(z)\n",
    "end\n",
    "\n",
    "function train(; kws...)\n",
    "    args = Args(; kws...)\n",
    "\n",
    "    args.seed > 0 && Random.seed!(args.seed)\n",
    "\n",
    "    if args.usecuda && CUDA.functional()\n",
    "        device = gpu\n",
    "        args.seed > 0 && CUDA.seed!(args.seed)\n",
    "        @info \"Training on GPU\"\n",
    "    else\n",
    "        device = cpu\n",
    "        @info \"Training on CPU\"\n",
    "    end\n",
    "\n",
    "    ### LOAD DATA\n",
    "    g = mldataset2gnngraph(PubMed())\n",
    "\n",
    "    # Print some info\n",
    "    display(g)\n",
    "    @show is_bidirected(g)\n",
    "    @show has_self_loops(g)\n",
    "    @show has_multi_edges(g)\n",
    "    @show mean(degree(g))\n",
    "    isbidir = is_bidirected(g)\n",
    "\n",
    "    # Move to device\n",
    "    g = g |> device\n",
    "    X = g.ndata.features\n",
    "\n",
    "    #### TRAIN/TEST splits\n",
    "    # With bidirected graph, we make sure that an edge and its reverse\n",
    "    # are in the same split \n",
    "    train_pos_g, test_pos_g = rand_edge_split(g, 0.9, bidirected = isbidir)\n",
    "    test_neg_g = negative_sample(g, num_neg_edges = test_pos_g.num_edges,\n",
    "                                 bidirected = isbidir)\n",
    "\n",
    "    ### DEFINE MODEL #########\n",
    "    nin, nhidden = size(X, 1), args.nhidden\n",
    "\n",
    "    # We embed the graph with positive training edges in the model \n",
    "    model = WithGraph(GNNChain(GCNConv(nin => nhidden, relu),\n",
    "                               GCNConv(nhidden => nhidden)),\n",
    "                      train_pos_g) |> device\n",
    "\n",
    "    pred = DotPredictor()\n",
    "\n",
    "    opt = Flux.setup(Adam(args.η), model)\n",
    "\n",
    "    ### LOSS FUNCTION ############\n",
    "\n",
    "    function loss(model, pos_g, neg_g = nothing; with_accuracy = false)\n",
    "        h = model(X)\n",
    "        if neg_g === nothing\n",
    "            # We sample a negative graph at each training step\n",
    "            neg_g = negative_sample(pos_g, bidirected = isbidir)\n",
    "        end\n",
    "        pos_score = pred(pos_g, h)\n",
    "        neg_score = pred(neg_g, h)\n",
    "        scores = [pos_score; neg_score]\n",
    "        labels = [fill!(similar(pos_score), 1); fill!(similar(neg_score), 0)]\n",
    "        l = logitbinarycrossentropy(scores, labels)\n",
    "        if with_accuracy\n",
    "            acc = 0.5 * mean(pos_score .>= 0) + 0.5 * mean(neg_score .< 0)\n",
    "            return l, acc\n",
    "        else\n",
    "            return l\n",
    "        end\n",
    "    end\n",
    "\n",
    "    ### LOGGING FUNCTION\n",
    "    function report(epoch)\n",
    "        train_loss, train_acc = loss(model, train_pos_g, with_accuracy = true)\n",
    "        test_loss, test_acc = loss(model, test_pos_g, test_neg_g, with_accuracy = true)\n",
    "        println(\"Epoch: $epoch  $((; train_loss, train_acc))  $((; test_loss, test_acc))\")\n",
    "    end\n",
    "\n",
    "    ### TRAINING\n",
    "    report(0)\n",
    "    for epoch in 1:(args.epochs)\n",
    "        grads = Flux.gradient(model -> loss(model, train_pos_g), model)\n",
    "        Flux.update!(opt, model, grads[1])\n",
    "        epoch % args.infotime == 0 && report(epoch)\n",
    "    end\n",
    "end\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
