{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of semi-supervised node classification\n",
    "\n",
    "using Flux\n",
    "using Flux: onecold, onehotbatch\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using GraphNeuralNetworks\n",
    "using MLDatasets: Cora\n",
    "using Statistics, Random\n",
    "using CUDA\n",
    "CUDA.allowscalar(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_loss_accuracy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function eval_loss_accuracy(X, y, mask, model, g)\n",
    "    ŷ = model(g, X)\n",
    "    l = logitcrossentropy(ŷ[:, mask], y[:, mask])\n",
    "    acc = mean(onecold(ŷ[:, mask]) .== onecold(y[:, mask]))\n",
    "    return (loss = round(l, digits = 4), acc = round(acc * 100, digits = 2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "  num_nodes: 2708\n",
       "  num_edges: 10556\n",
       "  ndata:\n",
       "\tval_mask = 2708-element Vector{Bool}\n",
       "\ttargets = 2708-element Vector{Int64}\n",
       "\ttest_mask = 2708-element Vector{Bool}\n",
       "\tfeatures = 1433×2708 Matrix{Float32}\n",
       "\ttrain_mask = 2708-element Vector{Bool}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Train: (loss = 1.9487f0, acc = 13.57)   Test: (loss = 1.9464f0, acc = 13.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training on CPU\n",
      "└ @ Main c:\\Users\\HP\\Desktop\\VicWorks03082024\\eSGCe_cora.ipynb:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10   Train: (loss = 1.541f0, acc = 94.29)   Test: (loss = 1.7289f0, acc = 69.0)\n",
      "Epoch: 20   Train: (loss = 0.8189f0, acc = 97.86)   Test: (loss = 1.2764f0, acc = 77.4)\n",
      "Epoch: 30   Train: (loss = 0.25f0, acc = 98.57)   Test: (loss = 0.8154f0, acc = 80.8)\n",
      "Epoch: 40   Train: (loss = 0.0615f0, acc = 100.0)   Test: (loss = 0.6465f0, acc = 79.4)\n",
      "Epoch: 50   Train: (loss = 0.0193f0, acc = 100.0)   Test: (loss = 0.6278f0, acc = 79.1)\n",
      "Epoch: 60   Train: (loss = 0.0089f0, acc = 100.0)   Test: (loss = 0.6436f0, acc = 79.2)\n",
      "Epoch: 70   Train: (loss = 0.0055f0, acc = 100.0)   Test: (loss = 0.6587f0, acc = 79.3)\n",
      "Epoch: 80   Train: (loss = 0.004f0, acc = 100.0)   Test: (loss = 0.6686f0, acc = 79.3)\n",
      "Epoch: 90   Train: (loss = 0.0032f0, acc = 100.0)   Test: (loss = 0.6752f0, acc = 79.3)\n",
      "Epoch: 100   Train: (loss = 0.0027f0, acc = 100.0)   Test: (loss = 0.6806f0, acc = 79.2)\n"
     ]
    }
   ],
   "source": [
    "# arguments for the `train` function \n",
    "Base.@kwdef mutable struct Args\n",
    "    η = 1.0f-3             # learning rate\n",
    "    epochs = 100          # number of epochs\n",
    "    seed = 17             # set seed > 0 for reproducibility\n",
    "    usecuda = true      # if true use cuda (if available)\n",
    "    nhidden = 128        # dimension of hidden features\n",
    "    infotime = 10      # report every `infotime` epochs\n",
    "end\n",
    "\n",
    "function train(; kws...)\n",
    "    args = Args(; kws...)\n",
    "\n",
    "    args.seed > 0 && Random.seed!(args.seed)\n",
    "\n",
    "    if args.usecuda && CUDA.functional()\n",
    "        device = gpu\n",
    "        args.seed > 0 && CUDA.seed!(args.seed)\n",
    "        @info \"Training on GPU\"\n",
    "    else\n",
    "        device = cpu\n",
    "        @info \"Training on CPU\"\n",
    "    end\n",
    "\n",
    "    # LOAD DATA\n",
    "    dataset = Cora()\n",
    "    classes = dataset.metadata[\"classes\"]\n",
    "    g = mldataset2gnngraph(dataset) |> device\n",
    "    X = g.features\n",
    "    y = onehotbatch(g.targets |> cpu, classes) |> device # remove when https://github.com/FluxML/Flux.jl/pull/1959 tagged\n",
    "    ytrain = y[:, g.train_mask]\n",
    "\n",
    "    nin, nhidden, nout = size(X, 1), args.nhidden, length(classes)\n",
    "\n",
    "    ## DEFINE MODEL\n",
    "    model = GNNChain(GCNConv(nin => nhidden, relu),\n",
    "                     GCNConv(nhidden => nhidden, relu),\n",
    "                     Dense(nhidden, nout)) |> device\n",
    "\n",
    "    opt = Flux.setup(Adam(args.η), model)\n",
    "\n",
    "    display(g)\n",
    "\n",
    "    ## LOGGING FUNCTION\n",
    "    function report(epoch)\n",
    "        train = eval_loss_accuracy(X, y, g.train_mask, model, g)\n",
    "        test = eval_loss_accuracy(X, y, g.test_mask, model, g)\n",
    "        println(\"Epoch: $epoch   Train: $(train)   Test: $(test)\")\n",
    "    end\n",
    "\n",
    "    ## TRAINING\n",
    "    report(0)\n",
    "    for epoch in 1:(args.epochs)\n",
    "        grad = Flux.gradient(model) do model\n",
    "            ŷ = model(g, X)\n",
    "            logitcrossentropy(ŷ[:, g.train_mask], ytrain)\n",
    "        end\n",
    "\n",
    "        Flux.update!(opt, model, grad[1])\n",
    "\n",
    "        epoch % args.infotime == 0 && report(epoch)\n",
    "    end\n",
    "end\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
